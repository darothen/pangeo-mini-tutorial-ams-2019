{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Moving from Single Jobs to Many Nodes: Dask, X-Array, and Pangeo, Part 2\n",
    "\n",
    "This is the second of a two notebook series which introduces the reader to basic concepts related to moving basic xarray workflows from single-machine to many-machine systems. This material is adapted from the excellent tutorial developed by [Ryan Abernathey, Joe Hamman, and Scott Henderson from the AGU 2018 Fall Meeting](https://github.com/pangeo-data/pangeo-tutorial-agu-2018/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Initial setup matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(['seaborn-ticks', 'seaborn-talk'])\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards many-core workflows\n",
    "\n",
    "One of the tricks that we have when working with `xarray` is native integration with **dask**, a distributing computing library which features an array object implementing the standard NumPy API; `dask.array`s are basically large arrays composed of many smaller NumPy arrays:\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a proper Dask workflow system isn't very complicated, but it's beyond the scope of this mini-tutorial. The Pangeo community has put together [several sets of documentation on how you can deploy a Dask system on both traditional HPC/cluster setups as well as on the cloud](http://pangeo.io/setup_guides/index.html).\n",
    "\n",
    "If you're running this notebook on Binder, we've already got you covered - we're using a tool called **dask-kubernetes**, which leans on a popular container orchestration tool for spinning up Dask workers as users need them. Normally, you can run the following code to set up and manually scale a **dask-kubernetes** `KubeCluster`:\n",
    "\n",
    "``` python\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=10)\n",
    "cluster\n",
    "```\n",
    "\n",
    "Due to an idiosyncrasy, we're going to manually make a cluster using the built-in Dask lab-extension:\n",
    "\n",
    "1. Click the dask icon on the left-hand panel (the orange wings)\n",
    "2. Click on \"+ NEW\"; this should create a \"KubeCluster\" in the panel underneath it\n",
    "3. Note the *Scheduler Address* (\"tcp://< ip >:< port >\"); we'll use that below.\n",
    "4. *Scale* the number of workers/cores on the cluster\n",
    "5. Create Dask Status board panes that you'd like to monitor before continuing.\n",
    "    \n",
    "Now, we can connect a client to the `KubeCluster`; be sure to fill in the \"ip\" and \"port\" strings below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "# If executing on Binder...\n",
    "# client = Client('<ip>:<port>')\n",
    "\n",
    "# ... else, if executing locally.\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dask array examples\n",
    "\n",
    "The following calculations give a feel for how Dask distributes array calculations. We can create a Dask array just like we might a NumPy array. The major difference is that we specify the *chunks* that we want to break the array into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((20000, 20000), chunks=(2000, 2000)).persist()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we print the Dask array, we get a heuristic explaining what the variable contains... not the values of the array itself. This is because Dask *defers* its calculations until the user tells it to do so. Intead, Dask is tracking a graph representing the calculation as it is built, and will try to optimize it before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0, :5].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have dask execute relatively aribtrarily complex calculations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T - x.mean(axis=0)\n",
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[::5, ::5].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask and xarray\n",
    "\n",
    "`xarray` has built-in functionality to lean on `dask`. Let's open our multi-file dataset one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = xr.open_mfdataset('../data/sst/*.nc')\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skipped over this point before, but look at the print out for the **sst** variable; note that it's not reporting any numerical vbalues, but instead is giving us a diagnostic similar to when we manually created Dask arrays earlier.\n",
    "\n",
    "By default, when we execute **open_mfdataset()**, xarray will instruct dask to try contain each separate file's contents as an individual chunk. Our dataset has 57 annual cycles of data, each contained in a single file, so xarray/dask automatically chunks the data like:\n",
    "\n",
    "    float dask.array<shape=(684, 89, 180), chunksize=(12, 89, 180)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete weighted average calculation from before, but observe what happens now that we have a chunked dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = ds_all.sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the grid cell areas like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6.37e6\n",
    "# we know already that the spacing of the points is one degree latitude\n",
    "dϕ = np.deg2rad(1.)\n",
    "dλ = np.deg2rad(1.)\n",
    "dA = R**2 * dϕ * dλ * np.cos(np.deg2rad(ds.lat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, plot the grid cell areas on a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_area = dA.where(sst[0].notnull())\n",
    "pixel_area.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute a timeseries of weighted global average SSTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ocean_area = pixel_area.sum(dim=('lon', 'lat'))\n",
    "sst_weighted_mean = (sst * pixel_area).sum(dim=('lon', 'lat')) / total_ocean_area\n",
    "sst_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_weighted_mean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 57 years of data instead of one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "\n",
    "Now that we have a bigger dataset, this is a good time to check out xarray's groupby capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_clim = ds_all.sst.groupby('time.month').mean(dim='time')\n",
    "sst_clim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data has dimension `month` instead of time!\n",
    "Each value represents the average among all of the Januaries, Februaries, etc. in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sst_clim[6] - sst_clim[0]).plot()\n",
    "plt.title('June minus July SST Climatology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample and Rolling\n",
    "\n",
    "Resample is meant specifically to work with time data (data with a `datetime64` variable as a dimension).\n",
    "It allows you to change the time-sampling frequency of your data.\n",
    "\n",
    "Let's illustrate by selecting a single point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ts = ds_all.sst.sel(lon=300, lat=10)\n",
    "sst_ts_annual = sst_ts.resample(time='A').mean(dim='time')\n",
    "sst_ts_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ts.plot()\n",
    "sst_ts_annual.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is a \"running mean\" over the time dimension.\n",
    "This can be accomplished with xarray's `.rolling` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ts_rolling = sst_ts.rolling(time=24).mean(dim='time', centered=True)\n",
    "sst_ts_annual.plot(marker='o')\n",
    "sst_ts_rolling.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
